from bs4 import BeautifulSoup
import time
import requests
import re
from urllib import parse
from multiprocessing.dummy import Pool
from plugin.config import baidu_header, thread_num, rule, page, delay


# è¿™éƒ¨åˆ†çˆ¬è™«å‚è€ƒäº†oneforallï¼Œå‘å¼€æºä½œè€…è¡¨ç¤ºæ„Ÿè°¢ã€‚https://github.com/shmilylty/OneForAll

class Baidu_spider(object):

    def __init__(self, keywords):
        self.keywords = keywords
        self.addr = 'https://www.baidu.com/s'
        # self.limit_num = 750  # é™åˆ¶æœç´¢æ¡æ•°
        self.header = baidu_header
        self.thread_num = thread_num
        self.rule = 1
        self.page_MAXNum = page
        self.delay = delay

    def get_resp(self,url):

        rep = requests.get(url,headers = baidu_header)
        if rep.status_code == 200:
            return rep.content
        else:return -1
    # è·å–é¡µé¢
    def get_data(self, keyword):
        # å•çº¿ç¨‹
        """

        :param keyword:
        :return:

        time.sleep(self.delay)
        resp_content = []
        # urlç¼–ç 
        # keyword = parse.quote(keyword)

        # æ‰“å°å…³é”®å­—
        # print(keyword)

        for page in range(self.page_MAXNum):
            params = {'wd': keyword,
                      'pn': page * 10,
                      'ie': 'utf-8'
                      }

            resp = requests.get(self.addr, params=params, headers=baidu_header)
            if resp.status_code == 200:
                resp_content.append(resp.content)
        """


        #å¤šçº¿ç¨‹
        resp_content = []
        url_list = []
        for page in range(self.page_MAXNum):
            params = self.addr+'?wd='+keyword+'&pn='+str(page*10)+'&ie=utf-8'
            url_list.append(params)
        #æ‰“å°è®¿é—®çš„url
        # print(url_list)
        #çº¿ç¨‹æ± 
        pool = Pool(processes=self.thread_num)

        resp_content = pool.map(self.get_resp,url_list)
        # print(resp_content)


        return resp_content


    # å¤„ç†è¿”å›åŒ…html,è¿”å›çœŸå®çš„urlåŠtitleåˆ—è¡¨
    def handle_data(self, resp):
        real_url_list = []
        title_list = []
        self.resp = resp
        if self.rule == 1:
            # todo å¤„ç†æ•°æ®
            soup = BeautifulSoup(self.resp, 'lxml')
            tagh3 = soup.find_all('h3')
            for h3 in tagh3:
                href = h3.find('a').get('href')
                # æ‰“å°baiduæœç´¢è·å–çš„é“¾æ¥
                # print(href)

                title = h3.find('a').strings  # éœ€è¦æ‹¼æ¥
                try:
                    baidu_url = requests.get(url=href, headers=self.header, allow_redirects=False)
                except:
                    str = 'è®¿é—®ä¸äº†' + href
                    # print(str)
                    real_url_list.append(str)
                real_url = baidu_url.headers['Location']  # å¾—åˆ°ç½‘é¡µåŸå§‹åœ°å€

                # æ‰“å°çœŸå®url
                # print(real_url)

                # all.write(real_url + '\n')
                real_url_list.append(real_url)
                # æ‹¼æ¥title
                _title = []
                for string in title:
                    # print(string)
                    comb = ''.join(string)
                    _title.append(comb)
                # æ‰“å°æ ‡é¢˜title
                # print(''.join(_title))
                title_list.append(''.join(_title))

            return real_url_list, title_list

    # çˆ¬è™«æ‰§è¡Œå…¥å£
    def run(self):
        result_list = []
        keyword = self.keywords
        print('baiduSpider Mod on ğŸ˜')

        # for i in range(self.page):
        #     data = self.get_data(self.keywords)
        #     result = self.handle_data(data)
        resp_text = self.get_data(keyword)
        for i in range(len(resp_text)):
            baidu_url_list, title_list = self.handle_data(resp_text[i])
            result_list.append(zip(baidu_url_list, title_list))
            # print(baidu_url_list,title_list)

        # æ‰“å°çˆ¬è™«æ•°æ®
        # for _ in range(len(result_list)):
        #     print(list(result_list[_]))

        if result_list:
            return result_list
        else:
            return -1


def do(keywords):
    spider = Baidu_spider(keywords=keywords)
    result = spider.run()
    return result


if __name__ == '__main__':
    sp = Baidu_spider(keywords='bbb')
    print(sp.run())
